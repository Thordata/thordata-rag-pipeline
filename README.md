# âš¡ Thordata RAG Pipeline

<div align="center">

**Intelligent Web Scraping & Question Answering System**  
**æ™ºèƒ½ç½‘é¡µæŠ“å–ä¸é—®ç­”ç³»ç»Ÿ**

*Powered by Thordata SDK + LangChain + ChromaDB*  
*åŸºäº Thordata SDK + LangChain + ChromaDB æ„å»º*

[![Python](https://img.shields.io/badge/Python-3.10%2B-blue)](https://www.python.org/)
[![Thordata SDK](https://img.shields.io/badge/Thordata%20SDK-1.8.4%2B-green)](https://github.com/Thordata/thordata-python-sdk)
[![License](https://img.shields.io/badge/license-MIT-green)](LICENSE)

</div>

---

## ğŸ“– Overview | æ¦‚è¿°

Thordata RAG Pipeline is a production-ready system that combines intelligent web scraping with Retrieval-Augmented Generation (RAG) capabilities. It automatically scrapes content from any URL, stores it in a vector database, and enables natural language question answering over the collected knowledge.

**Thordata RAG Pipeline** æ˜¯ä¸€ä¸ªç”Ÿäº§å°±ç»ªçš„ç³»ç»Ÿï¼Œç»“åˆäº†æ™ºèƒ½ç½‘é¡µæŠ“å–å’Œæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰èƒ½åŠ›ã€‚å®ƒå¯ä»¥è‡ªåŠ¨ä»ä»»ä½• URL æŠ“å–å†…å®¹ï¼Œå­˜å‚¨åˆ°å‘é‡æ•°æ®åº“ä¸­ï¼Œå¹¶æ”¯æŒå¯¹æ”¶é›†çš„çŸ¥è¯†è¿›è¡Œè‡ªç„¶è¯­è¨€é—®ç­”ã€‚

### What is RAG? | ä»€ä¹ˆæ˜¯ RAGï¼Ÿ

**RAG** (Retrieval-Augmented Generation) = **æ£€ç´¢å¢å¼ºç”Ÿæˆ**

Think of it as a super assistant that:
- Takes a URL, "reads" the webpage, and memorizes important content
- Organizes and stores the content in a "knowledge base"
- Answers your questions based on the stored knowledge

**RAG** å°±åƒä¸€ä¸ªè¶…çº§åŠ©æ‰‹ï¼š
- ä½ ç»™å®ƒä¸€ä¸ªç½‘å€ï¼Œå®ƒå»"çœ‹"è¿™ä¸ªç½‘é¡µï¼ŒæŠŠé‡è¦å†…å®¹è®°ä¸‹æ¥
- å®ƒæŠŠçœ‹åˆ°çš„å†…å®¹æ•´ç†å¥½ï¼Œå­˜åˆ°ä¸€ä¸ª"çŸ¥è¯†åº“"é‡Œ
- ä¹‹åä½ é—®å®ƒä»»ä½•å…³äºè¿™äº›å†…å®¹çš„é—®é¢˜ï¼Œå®ƒéƒ½èƒ½ä»"çŸ¥è¯†åº“"é‡Œæ‰¾åˆ°ç­”æ¡ˆå‘Šè¯‰ä½ 

**Why RAG? | ä¸ºä»€ä¹ˆéœ€è¦ RAGï¼Ÿ**
- AI models have limited knowledge and may be outdated
- RAG allows AI to answer questions about the latest web content
- Like giving AI a "real-time updated encyclopedia"

**ä¸ºä»€ä¹ˆéœ€è¦ RAGï¼Ÿ**
- AI æ¨¡å‹æœ¬èº«çš„çŸ¥è¯†æœ‰é™ï¼Œè€Œä¸”å¯èƒ½è¿‡æ—¶
- é€šè¿‡ RAGï¼Œå¯ä»¥è®© AI å›ç­”å…³äºæœ€æ–°ç½‘é¡µå†…å®¹çš„é—®é¢˜
- å°±åƒç»™ AI é…äº†ä¸€ä¸ª"å®æ—¶æ›´æ–°çš„ç™¾ç§‘å…¨ä¹¦"

### Key Highlights | æ ¸å¿ƒäº®ç‚¹

- âœ… **107+ Auto-Discovered Spiders | 107+ è‡ªåŠ¨å‘ç°çˆ¬è™«** - No manual configuration needed | æ— éœ€æ‰‹åŠ¨é…ç½®
- âœ… **Free LLM Support | å…è´¹ LLM æ”¯æŒ** - Works with SiliconFlow free models | æ”¯æŒç¡…åŸºæµåŠ¨å…è´¹æ¨¡å‹  
- âœ… **One-Command Usage | ä¸€é”®ä½¿ç”¨** - `quick_start.py` for instant results | `quick_start.py` å³æ—¶ç»“æœ
- âœ… **Production Ready | ç”Ÿäº§å°±ç»ª** - Async, cached, monitored | å¼‚æ­¥ã€ç¼“å­˜ã€ç›‘æ§
- âœ… **Smart Routing | æ™ºèƒ½è·¯ç”±** - Automatically selects best scraping strategy | è‡ªåŠ¨é€‰æ‹©æœ€ä½³æŠ“å–ç­–ç•¥

---

## âœ¨ Key Features | æ ¸å¿ƒç‰¹æ€§

### ğŸ§  Smart Routing | æ™ºèƒ½è·¯ç”±
Automatically selects the best scraping strategy (specialized spiders vs. universal scraper)  
è‡ªåŠ¨é€‰æ‹©æœ€ä½³æŠ“å–ç­–ç•¥ï¼ˆä¸“ä¸šçˆ¬è™« vs é€šç”¨çˆ¬è™«ï¼‰

**How it works | å·¥ä½œåŸç†ï¼š**
- Analyzes URL to determine website type
- Uses specialized spiders for known sites (Amazon, YouTube, etc.)
- Falls back to universal scraper for unknown sites
- åˆ†æç½‘å€åˆ¤æ–­ç½‘ç«™ç±»å‹
- å¯¹å·²çŸ¥ç½‘ç«™ä½¿ç”¨ä¸“ä¸šçˆ¬è™«ï¼ˆAmazonã€YouTube ç­‰ï¼‰
- å¯¹æœªçŸ¥ç½‘ç«™ä½¿ç”¨é€šç”¨çˆ¬è™«

### ğŸ•·ï¸ 107+ Auto-Discovered Spiders | 107+ è‡ªåŠ¨å‘ç°çˆ¬è™«
Automatically discovers and uses all available spiders from Thordata SDK  
è‡ªåŠ¨å‘ç°å¹¶ä½¿ç”¨ Thordata SDK ä¸­çš„æ‰€æœ‰å¯ç”¨çˆ¬è™«

**Supported Platforms | æ”¯æŒçš„å¹³å°ï¼š**
- **E-Commerce | ç”µå•†**: Amazon, Google Shopping, TikTok Shop
- **Social Media | ç¤¾äº¤åª’ä½“**: YouTube, TikTok, Instagram, Facebook, Twitter, Reddit, LinkedIn
- **Maps & Stores | åœ°å›¾å’Œå•†åº—**: Google Maps, Google Play Store
- **Code Platforms | ä»£ç å¹³å°**: GitHub
- **Universal | é€šç”¨**: Any other website

### ğŸŒ Universal Scraper | é€šç”¨çˆ¬è™«
Fallback to headless browser scraping for any website  
å¯¹ä»»ä½•ç½‘ç«™çš„åå¤‡æ— å¤´æµè§ˆå™¨æŠ“å–

### ğŸ“š Vector Storage | å‘é‡å­˜å‚¨
ChromaDB integration for semantic search  
ChromaDB é›†æˆç”¨äºè¯­ä¹‰æœç´¢

**What is Vector Store? | ä»€ä¹ˆæ˜¯å‘é‡å­˜å‚¨ï¼Ÿ**
- Converts text into numerical vectors (e.g., [0.1, 0.5, -0.3, ...])
- Similar texts have similar vectors
- Enables fast similarity search
- æŠŠæ–‡å­—è½¬æ¢æˆæ•°å­—åˆ—è¡¨ï¼ˆå‘é‡ï¼‰
- ç›¸ä¼¼çš„æ–‡å­—ä¼šæœ‰ç›¸ä¼¼çš„æ•°å­—åˆ—è¡¨
- æ”¯æŒå¿«é€Ÿç›¸ä¼¼åº¦æœç´¢

### ğŸ’¬ RAG Q&A | RAG é—®ç­”
Ask questions about scraped content using LLMs  
ä½¿ç”¨å¤§è¯­è¨€æ¨¡å‹å¯¹æŠ“å–å†…å®¹è¿›è¡Œé—®ç­”

### âš¡ Async-First | å¼‚æ­¥ä¼˜å…ˆ
Built on async/await for high performance  
åŸºäº async/await æ„å»ºï¼Œæ€§èƒ½ä¼˜å¼‚

### ğŸ’¾ Intelligent Caching | æ™ºèƒ½ç¼“å­˜
In-memory cache to avoid redundant scraping  
å†…å­˜ç¼“å­˜é¿å…é‡å¤æŠ“å–

### ğŸ”„ Batch Processing | æ‰¹é‡å¤„ç†
Process multiple URLs concurrently  
å¹¶å‘å¤„ç†å¤šä¸ª URL

### ğŸ”Œ Multi-Provider LLM Support | å¤šæä¾›å•† LLM æ”¯æŒ
Auto-detects models for SiliconFlow, OpenAI, DeepSeek, etc.  
è‡ªåŠ¨æ£€æµ‹ç¡…åŸºæµåŠ¨ã€OpenAIã€DeepSeek ç­‰æ¨¡å‹çš„é…ç½®

**Recommended Free Models | æ¨èçš„å…è´¹æ¨¡å‹ï¼š**
- **LLM**: `Qwen/Qwen2.5-7B-Instruct` (via SiliconFlow)
- **Embedding**: `BAAI/bge-large-zh-v1.5` (via SiliconFlow)

### âœ… Production Tested | ç”Ÿäº§æµ‹è¯•
Fully tested and verified with real APIs  
å·²ä½¿ç”¨çœŸå® API å®Œæ•´æµ‹è¯•éªŒè¯

---

## ğŸš€ Quick Start | å¿«é€Ÿå¼€å§‹

### Prerequisites | å‰ç½®è¦æ±‚

- Python 3.10 or higher | Python 3.10 æˆ–æ›´é«˜ç‰ˆæœ¬
- Thordata API credentials | Thordata API å‡­è¯
- LLM API key (SiliconFlow recommended for free models) | LLM API å¯†é’¥ï¼ˆæ¨èç¡…åŸºæµåŠ¨å…è´¹æ¨¡å‹ï¼‰

### Installation | å®‰è£…

#### 1. Clone the repository | å…‹éš†ä»“åº“

```bash
git clone https://github.com/Thordata/thordata-rag-pipeline.git
cd thordata-rag-pipeline
```

#### 2. Install dependencies | å®‰è£…ä¾èµ–

```bash
pip install -r requirements.txt
```

#### 3. Verify setup | éªŒè¯å®‰è£…

```bash
python check_setup.py
```

You should see "Setup complete! You can now use the pipeline."  
ä½ åº”è¯¥çœ‹åˆ° "Setup complete! You can now use the pipeline."

#### 4. Configure credentials | é…ç½®å‡­è¯

Copy `.env.example` to `.env` and fill in your credentials:  
å¤åˆ¶ `.env.example` åˆ° `.env` å¹¶å¡«å†™æ‚¨çš„å‡­è¯ï¼š

```bash
cp .env.example .env
# Edit .env with your credentials
```

**Required configuration | å¿…éœ€é…ç½®ï¼š**

```ini
# Thordata Credentials | Thordata å‡­è¯
THORDATA_SCRAPER_TOKEN=your_token_here
THORDATA_PUBLIC_TOKEN=your_token_here
THORDATA_PUBLIC_KEY=your_key_here

# LLM Configuration | LLM é…ç½®
# For SiliconFlow (Free) | ç¡…åŸºæµåŠ¨ï¼ˆå…è´¹ï¼‰
OPENAI_API_KEY=your_siliconflow_api_key
OPENAI_API_BASE=https://api.siliconflow.cn/v1
OPENAI_MODEL=Qwen/Qwen2.5-7B-Instruct
OPENAI_EMBEDDING_MODEL=BAAI/bge-large-zh-v1.5
```

**Where to get credentials | å¦‚ä½•è·å–å‡­è¯ï¼š**
- **Thordata**: Get from [Thordata website](https://thordata.com) | ä» [Thordata å®˜ç½‘](https://thordata.com) è·å–
- **SiliconFlow**: Get free API key from [SiliconFlow](https://siliconflow.cn) | ä» [ç¡…åŸºæµåŠ¨](https://siliconflow.cn) è·å–å…è´¹ API å¯†é’¥

---

## ğŸ“š Usage | ä½¿ç”¨æ–¹æ³•

### Method 1: Quick Start (Simplest) | æ–¹æ³•ä¸€ï¼šå¿«é€Ÿå¼€å§‹ï¼ˆæœ€ç®€å•ï¼‰

One command to scrape and answer:  
ä¸€æ¡å‘½ä»¤å®ŒæˆæŠ“å–å’Œå›ç­”ï¼š

```bash
python quick_start.py "https://example.com" "What is this website about?"
python quick_start.py "https://example.com" "è¿™ä¸ªç½‘ç«™è®²ä»€ä¹ˆï¼Ÿ"
```

**What it does | ç³»ç»Ÿä¼šï¼š**
1. Scrape the webpage | æŠ“å–ç½‘é¡µå†…å®¹
2. Store in vector database | å­˜å‚¨åˆ°å‘é‡æ•°æ®åº“
3. Answer your question | å›ç­”ä½ çš„é—®é¢˜

### Method 2: Full Pipeline | æ–¹æ³•äºŒï¼šå®Œæ•´æµç¨‹

#### Scrape and Answer | æŠ“å–å¹¶å›ç­”é—®é¢˜

```bash
python main.py --url "https://example.com" --question "What is this about?"
python main.py --url "https://example.com" --question "è¿™ä¸ªç½‘ç«™è®²ä»€ä¹ˆï¼Ÿ"
```

#### Ingest Only | ä»…æŠ“å–

```bash
python main.py --url "https://example.com" --ingest-only
```

**Use cases | é€‚ç”¨åœºæ™¯ï¼š**
- Collect content first, ask questions later | æƒ³å…ˆæ”¶é›†å†…å®¹ï¼Œç¨åå†é—®é—®é¢˜
- Batch scraping multiple websites | æ‰¹é‡æŠ“å–å¤šä¸ªç½‘ç«™

#### Query Only | ä»…æŸ¥è¯¢

```bash
python main.py --question "What did we learn about Python?" --query-only
python main.py --question "ä¹‹å‰æŠ“å–çš„å†…å®¹ä¸­ï¼ŒPython çš„ç‰¹ç‚¹æ˜¯ä»€ä¹ˆï¼Ÿ" --query-only
```

**Use cases | é€‚ç”¨åœºæ™¯ï¼š**
- Already scraped content | å·²ç»æŠ“å–è¿‡å†…å®¹
- Query existing knowledge base | æƒ³æŸ¥è¯¢ä¹‹å‰å­˜å‚¨çš„çŸ¥è¯†

#### Batch Processing | æ‰¹é‡å¤„ç†

```bash
python main.py --urls "https://example.com,https://another.com,https://third.com" --ingest-only
```

**Note | æ³¨æ„**: URLs separated by commas, no spaces | ç½‘å€ä¹‹é—´ç”¨é€—å·åˆ†éš”ï¼Œä¸è¦æœ‰ç©ºæ ¼

#### Advanced Options | é«˜çº§é€‰é¡¹

```bash
# Disable cache | ç¦ç”¨ç¼“å­˜
python main.py --url "https://example.com" --no-cache

# Specify number of documents to retrieve (default: 5) | æŒ‡å®šæ£€ç´¢çš„æ–‡æ¡£æ•°é‡ï¼ˆé»˜è®¤5ä¸ªï¼‰
python main.py --url "https://example.com" --question "Question" --k 10

# Clear cache | æ¸…é™¤ç¼“å­˜
python main.py --url "https://example.com" --clear-cache
```

---

## ğŸ”§ How It Works | å·¥ä½œåŸç†

### Complete Pipeline | å®Œæ•´æµç¨‹

```
1. Input URL and question | è¾“å…¥ç½‘å€å’Œé—®é¢˜
   â†“
2. Smart routing selects scraping method | æ™ºèƒ½è·¯ç”±é€‰æ‹©æŠ“å–æ–¹å¼
   â”œâ”€ Specialized spider (Amazon, YouTube, etc.) | ä¸“ä¸šçˆ¬è™«
   â””â”€ Universal scraper (other websites) | é€šç”¨çˆ¬è™«
   â†“
3. Scrape webpage content | æŠ“å–ç½‘é¡µå†…å®¹
   â†“
4. Chunk content (split into segments) | å†…å®¹åˆ†å—
   â†“
5. Convert to vectors (using embedding model) | è½¬æ¢ä¸ºå‘é‡
   â†“
6. Store in vector database | å­˜å‚¨åˆ°å‘é‡æ•°æ®åº“
   â†“
7. Search relevant content (vector similarity) | æœç´¢ç›¸å…³å†…å®¹
   â†“
8. Generate answer (using LLM) | AI ç”Ÿæˆç­”æ¡ˆ
   â†“
9. Return answer | è¿”å›ç­”æ¡ˆ
```

### Key Components | æ ¸å¿ƒç»„ä»¶

#### 1. Smart Router | æ™ºèƒ½è·¯ç”±
- Analyzes URL patterns | åˆ†æç½‘å€æ¨¡å¼
- Selects best scraping strategy | é€‰æ‹©æœ€ä½³æŠ“å–ç­–ç•¥
- Falls back automatically | è‡ªåŠ¨é™çº§å¤„ç†

#### 2. Document Chunker | æ–‡æ¡£åˆ†å—
- Splits long content into manageable chunks | å°†é•¿å†…å®¹åˆ†å‰²æˆå¯ç®¡ç†çš„å—
- Optimized chunk size: 400 characters (for 512 token limit) | ä¼˜åŒ–å—å¤§å°ï¼š400 å­—ç¬¦ï¼ˆé€‚é… 512 token é™åˆ¶ï¼‰
- Overlap: 50 characters (prevents information loss) | é‡å ï¼š50 å­—ç¬¦ï¼ˆé¿å…ä¿¡æ¯ä¸¢å¤±ï¼‰

#### 3. Vector Store | å‘é‡å­˜å‚¨
- **Embedding Model**: `BAAI/bge-large-zh-v1.5` (Chinese-optimized) | ä¸­æ–‡ä¼˜åŒ–æ¨¡å‹
- **Database**: ChromaDB (local storage) | æœ¬åœ°å­˜å‚¨
- **Vector Dimension**: 1024 | å‘é‡ç»´åº¦ï¼š1024

#### 4. LLM Query | LLM æŸ¥è¯¢
- **Model**: `Qwen/Qwen2.5-7B-Instruct` (Free via SiliconFlow) | å…è´¹æ¨¡å‹ï¼ˆé€šè¿‡ç¡…åŸºæµåŠ¨ï¼‰
- **Process**: Retrieves relevant chunks â†’ Generates answer | æ£€ç´¢ç›¸å…³å— â†’ ç”Ÿæˆç­”æ¡ˆ

---

## ğŸ“Š Performance | æ€§èƒ½æŒ‡æ ‡

- **Scraping Speed**: ~10-15 seconds (large pages) | æŠ“å–é€Ÿåº¦ï¼š~10-15 ç§’ï¼ˆå¤§ç½‘é¡µï¼‰
- **Embedding Speed**: ~0.5 seconds per batch (30 chunks) | å‘é‡åŒ–é€Ÿåº¦ï¼š~0.5 ç§’/æ‰¹æ¬¡ï¼ˆ30 ä¸ªæ–‡æ¡£å—ï¼‰
- **LLM Response**: ~1-2 seconds | AI å“åº”ï¼š~1-2 ç§’
- **Total Pipeline**: ~15-20 seconds (full RAG cycle) | å®Œæ•´æµç¨‹ï¼š~15-20 ç§’

---

## â“ FAQ | å¸¸è§é—®é¢˜

### 1. Why do I need Thordata credentials? | ä¸ºä»€ä¹ˆéœ€è¦ Thordata å¯†é’¥ï¼Ÿ

**Answer | ç­”**ï¼šThordata provides professional web scraping services:
- Anti-bot bypass | åçˆ¬è™«ç»•è¿‡
- JavaScript rendering | JavaScript æ¸²æŸ“
- IP proxy | IP ä»£ç†
- 107+ specialized spiders | 107+ ä¸“ä¸šçˆ¬è™«

These services require paid usage (pay-per-use).  
è¿™äº›æœåŠ¡éœ€è¦ä»˜è´¹ä½¿ç”¨ï¼ˆæŒ‰ä½¿ç”¨é‡è®¡è´¹ï¼‰ã€‚

### 2. Why do I need LLM API key? | ä¸ºä»€ä¹ˆéœ€è¦ LLM API å¯†é’¥ï¼Ÿ

**Answer | ç­”**ï¼šAI models require computational resources. Even with free models (like SiliconFlow), you need:
- API key for authentication | API å¯†é’¥éªŒè¯èº«ä»½
- Server resources to process requests | æœåŠ¡å™¨èµ„æºå¤„ç†è¯·æ±‚

**Good news | å¥½æ¶ˆæ¯**ï¼šSiliconFlow provides free quota, sufficient for daily use.  
ç¡…åŸºæµåŠ¨æä¾›å…è´¹é¢åº¦ï¼Œè¶³å¤Ÿæ—¥å¸¸ä½¿ç”¨ã€‚

### 3. Where is data stored? | æ•°æ®å­˜å‚¨åœ¨å“ªé‡Œï¼Ÿ

**Answer | ç­”**ï¼š
- **Vector Database**: `./data/chroma_db/` (local folder) | æœ¬åœ°æ–‡ä»¶å¤¹
- **Cache**: In memory (cleared when program closes) | å†…å­˜ä¸­ï¼ˆç¨‹åºå…³é—­åæ¶ˆå¤±ï¼‰
- **Original Content**: Not saved, only processed vectors | ä¸ä¿å­˜ï¼Œåªä¿å­˜å¤„ç†åçš„å‘é‡

**âš ï¸ Important | é‡è¦**ï¼š`.env` file contains keys - **DO NOT** upload to GitHub!  
`.env` æ–‡ä»¶åŒ…å«å¯†é’¥ï¼Œ**ä¸è¦**ä¸Šä¼ åˆ° GitHubï¼

### 4. Which websites can I scrape? | å¯ä»¥æŠ“å–å“ªäº›ç½‘ç«™ï¼Ÿ

**Answer | ç­”**ï¼š
- **Specialized Support**: Amazon, YouTube, TikTok, Instagram, etc. (107+ websites) | 107+ ç§ç½‘ç«™
- **Universal Support**: Any publicly accessible website | ä»»ä½•å¯å…¬å¼€è®¿é—®çš„ç½‘ç«™

**Limitations | é™åˆ¶**ï¼š
- Website must be publicly accessible | éœ€è¦ç½‘ç«™å¯å…¬å¼€è®¿é—®
- Some websites may have access restrictions | æŸäº›ç½‘ç«™å¯èƒ½æœ‰è®¿é—®é™åˆ¶
- Scraping speed depends on website response time | æŠ“å–é€Ÿåº¦å–å†³äºç½‘ç«™å“åº”é€Ÿåº¦

### 5. Does scraped content expire? | æŠ“å–çš„å†…å®¹ä¼šè¿‡æœŸå—ï¼Ÿ

**Answer | ç­”**ï¼š
- Content in vector database does not auto-update | å‘é‡æ•°æ®åº“ä¸­çš„å†…å®¹ä¸ä¼šè‡ªåŠ¨æ›´æ–°
- If webpage content updates, re-scrape is needed | å¦‚æœç½‘é¡µå†…å®¹æ›´æ–°äº†ï¼Œéœ€è¦é‡æ–°æŠ“å–
- You can periodically run scraping commands to update | å¯ä»¥å®šæœŸè¿è¡ŒæŠ“å–å‘½ä»¤æ›´æ–°å†…å®¹

### 6. How long can a webpage be? | å¯ä»¥å¤„ç†å¤šé•¿çš„ç½‘é¡µï¼Ÿ

**Answer | ç­”**ï¼š
- Default: Max 50,000 characters | é»˜è®¤æœ€å¤šå¤„ç† 50,000 ä¸ªå­—ç¬¦
- Excess content will be truncated | è¶…è¿‡éƒ¨åˆ†ä¼šè¢«æˆªæ–­
- Can modify `MAX_CONTENT_LENGTH` in `.env` | å¯ä»¥åœ¨ `.env` ä¸­ä¿®æ”¹ `MAX_CONTENT_LENGTH`

### 7. Why are answers inaccurate? | ä¸ºä»€ä¹ˆå›ç­”ä¸å‡†ç¡®ï¼Ÿ

**Possible reasons | å¯èƒ½åŸå› **ï¼š
1. **Insufficient scraped content**: Too little content or scraping failed | æŠ“å–çš„å†…å®¹ä¸å¤Ÿ
2. **Vague question**: Question too broad | é—®é¢˜ä¸å¤Ÿå…·ä½“
3. **Content not in knowledge base**: What you're asking wasn't scraped | ç›¸å…³å†…å®¹ä¸åœ¨çŸ¥è¯†åº“

**Solutions | è§£å†³æ–¹æ³•**ï¼š
- Check scraped content length (should be > 200 chars) | æ£€æŸ¥æŠ“å–çš„å†…å®¹é•¿åº¦
- Make questions specific and clear | é—®é¢˜è¦å…·ä½“æ˜ç¡®
- Ensure relevant webpages have been scraped | ç¡®ä¿ç›¸å…³ç½‘é¡µå·²ç»è¢«æŠ“å–

### 8. Can I process multiple URLs at once? | å¯ä»¥åŒæ—¶å¤„ç†å¤šä¸ªç½‘å€å—ï¼Ÿ

**Answer | ç­”**ï¼šYes! Use batch mode:  
å¯ä»¥ï¼ä½¿ç”¨æ‰¹é‡æ¨¡å¼ï¼š

```bash
python main.py --urls "url1,url2,url3" --ingest-only
```

System processes concurrently for efficiency.  
ç³»ç»Ÿä¼šå¹¶å‘å¤„ç†ï¼Œæé«˜æ•ˆç‡ã€‚

### 9. How do I check stored content? | å¦‚ä½•æŸ¥çœ‹å­˜å‚¨äº†å¤šå°‘å†…å®¹ï¼Ÿ

**Answer | ç­”**ï¼šAfter running commands, statistics are displayed:  
è¿è¡Œå‘½ä»¤åï¼Œæœ€åä¼šæ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯ï¼š

```
[STATS]
  Documents in vector store: 222
  Cached items: 1
```

### 10. How do I clear stored content? | å¦‚ä½•æ¸…é™¤å­˜å‚¨çš„å†…å®¹ï¼Ÿ

**Answer | ç­”**ï¼š
- Delete `./data/chroma_db/` folder | åˆ é™¤ `./data/chroma_db/` æ–‡ä»¶å¤¹
- New database will be created automatically on next run | é‡æ–°è¿è¡Œç¨‹åºä¼šè‡ªåŠ¨åˆ›å»ºæ–°çš„æ•°æ®åº“

---

## ğŸ› ï¸ Troubleshooting | æ•…éšœæ’é™¤

### Embedding Model Issues | åµŒå…¥æ¨¡å‹é—®é¢˜

If you encounter "Model does not exist" errors with SiliconFlow:  
å¦‚æœä½¿ç”¨ SiliconFlow æ—¶é‡åˆ°"æ¨¡å‹ä¸å­˜åœ¨"é”™è¯¯ï¼š

**Solution | è§£å†³æ–¹æ¡ˆ**ï¼šSet a valid embedding model in `.env`:  
åœ¨ `.env` ä¸­è®¾ç½®æœ‰æ•ˆçš„åµŒå…¥æ¨¡å‹ï¼š

```ini
OPENAI_EMBEDDING_MODEL=BAAI/bge-large-zh-v1.5
```

### Import Errors | å¯¼å…¥é”™è¯¯

```bash
# Install missing dependencies | å®‰è£…ç¼ºå¤±çš„ä¾èµ–
pip install -r requirements.txt
```

### Configuration Errors | é…ç½®é”™è¯¯

```bash
# Check your configuration | æ£€æŸ¥é…ç½®
python check_setup.py
```

### Task Failures | ä»»åŠ¡å¤±è´¥

- Check Thordata credentials in `.env` | æ£€æŸ¥ `.env` ä¸­çš„ Thordata å‡­è¯
- Verify URL is accessible | éªŒè¯ URL å¯è®¿é—®
- Check task status in Thordata dashboard | åœ¨ Thordata ä»ªè¡¨æ¿ä¸­æ£€æŸ¥ä»»åŠ¡çŠ¶æ€

---

## ğŸ“ Project Structure | é¡¹ç›®ç»“æ„

```
thordata-rag-pipeline/
â”œâ”€â”€ .env.example          # Configuration template | é…ç½®æ¨¡æ¿
â”œâ”€â”€ .gitignore            # Git ignore file | Git å¿½ç•¥æ–‡ä»¶
â”œâ”€â”€ CHANGELOG.md          # Changelog | å˜æ›´æ—¥å¿—
â”œâ”€â”€ README.md             # This file | æœ¬æ–‡ä»¶
â”œâ”€â”€ requirements.txt      # Dependencies | ä¾èµ–åˆ—è¡¨
â”œâ”€â”€ check_setup.py        # Setup verification | ç¯å¢ƒæ£€æŸ¥è„šæœ¬
â”œâ”€â”€ main.py              # Main entry point | ä¸»ç¨‹åºå…¥å£
â”œâ”€â”€ quick_start.py       # Quick start script | å¿«é€Ÿå¼€å§‹è„šæœ¬
â”œâ”€â”€ src/                 # Source code | æºä»£ç 
â”‚   â””â”€â”€ thordata_rag/
â”‚       â”œâ”€â”€ core/        # Core configuration | æ ¸å¿ƒé…ç½®
â”‚       â”œâ”€â”€ ingestors/   # Scraping modules | æŠ“å–æ¨¡å—
â”‚       â””â”€â”€ processor/   # Processing modules | å¤„ç†æ¨¡å—
â””â”€â”€ tests/               # Test files | æµ‹è¯•æ–‡ä»¶
```

---

## ğŸ¤ Contributing | è´¡çŒ®

Contributions are welcome! Please feel free to submit a Pull Request.  
æ¬¢è¿è´¡çŒ®ï¼è¯·éšæ—¶æäº¤ Pull Requestã€‚

---

## ğŸ“„ License | è®¸å¯è¯

This project is licensed under the MIT License. See the [LICENSE](LICENSE) file for details.  
æœ¬é¡¹ç›®é‡‡ç”¨ MIT è®¸å¯è¯ã€‚è¯¦æƒ…è¯·å‚é˜… [LICENSE](LICENSE) æ–‡ä»¶ã€‚

---

## ğŸ™ Acknowledgments | è‡´è°¢

- [Thordata Python SDK](https://github.com/Thordata/thordata-python-sdk) - Web scraping infrastructure | ç½‘é¡µæŠ“å–åŸºç¡€è®¾æ–½
- [LangChain](https://github.com/langchain-ai/langchain) - LLM framework | LLM æ¡†æ¶
- [ChromaDB](https://www.trychroma.com/) - Vector database | å‘é‡æ•°æ®åº“
- [SiliconFlow](https://siliconflow.cn) - Free LLM API | å…è´¹ LLM API

---

## ğŸ“ Support | æ”¯æŒ

For detailed usage instructions, see the [Complete User Guide](../thordata-rag-pipeline-å®Œæ•´ä½¿ç”¨æŒ‡å—.md) (Chinese).  
è¯¦ç»†ä½¿ç”¨è¯´æ˜ï¼Œè¯·å‚é˜…[å®Œæ•´ä½¿ç”¨æŒ‡å—](../thordata-rag-pipeline-å®Œæ•´ä½¿ç”¨æŒ‡å—.md)ï¼ˆä¸­æ–‡ï¼‰ã€‚

For issues and questions, please open an [Issue](https://github.com/Thordata/thordata-rag-pipeline/issues).  
å¦‚æœ‰é—®é¢˜å’Œç–‘é—®ï¼Œè¯·æäº¤ [Issue](https://github.com/Thordata/thordata-rag-pipeline/issues)ã€‚

---

<div align="center">

**Made with â¤ï¸ by Thordata**  
**ç”± Thordata ç”¨ â¤ï¸ åˆ¶ä½œ**

</div>
